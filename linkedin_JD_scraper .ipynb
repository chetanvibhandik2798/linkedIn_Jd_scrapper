{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385ad0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started scrapping for url : \"Training\" or \"trainer\" with location United States\n",
      "scrolling\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import re\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def chrome_browser():\n",
    "    chrome_service = ChromeService(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=chrome_service)\n",
    "    return driver \n",
    "\n",
    "#__________________________________________________________________________________________________________________________#\n",
    "\n",
    "base_url = 'https://www.linkedin.com/jobs/search/?position=1&pageNum=0'\n",
    "\n",
    "#___________________________________________________________________________________________________________________________#\n",
    "class linked_in_JobScraper():\n",
    "    def __init__(self):\n",
    "        self.browser = chrome_browser()\n",
    "        self.links_processed = 0\n",
    "        self.links_threshold = random.randint(520,680)\n",
    "        self.pause_duration = random.randint(1*60,3*60)\n",
    "        self.nt_parsed_link = []\n",
    "        self.job_data_df = pd.DataFrame(columns=[\"job_title\", \"job_description\", \"organization_name\", \n",
    "                      \"location\", \"department\", \"key_skills\", \"seniority_level\", \n",
    "                      \"employment_type\", \"industries\", \"job_function\", \"job_link\",\"source\",\"searched_key_ward\"])\n",
    "        self.source = \"linkedin\"\n",
    "        self.target_button_class = \"infinite-scroller__show-more-button--visible\"\n",
    "        \n",
    "          \n",
    "    def job_scrapper(self,job_link,key_words):\n",
    "        job_dict = {}\n",
    "        retries = 0\n",
    "        max_retries = random.randint(5,8)\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                job_title_element = WebDriverWait(self.browser,2).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, 'h2.top-card-layout__title, h1.top-card-layout__title')))\n",
    "                job_title = job_title_element.text if job_title_element else None\n",
    "                job_dict[\"job_title\"] = str(job_title)\n",
    "\n",
    "                job_description_element = self.browser.find_element(By.CSS_SELECTOR,\"div.show-more-less-html__markup\")\n",
    "                job_description = job_description_element.get_attribute(\"innerHTML\")\n",
    "                job_dict[\"job_description\"] = str(job_description)\n",
    "\n",
    "                org_name_element = WebDriverWait(self.browser,2).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, 'a.topcard__org-name-link.topcard__flavor--black-link')))\n",
    "                org_name_text = org_name_element.text if org_name_element else None\n",
    "                job_dict[\"organization_name\"] = str(org_name_text)\n",
    "\n",
    "                location_element = WebDriverWait(self.browser,2).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, 'span.topcard__flavor.topcard__flavor--bullet')))\n",
    "                location_text = location_element.text if location_element else None\n",
    "                job_dict[\"location\"] = str(location_text)\n",
    "\n",
    "                Job_details  = self.browser.find_element(By.CLASS_NAME,\"description__job-criteria-list\")\n",
    "                Job_details = Job_details.text if Job_details else None\n",
    "                if Job_details: \n",
    "                    Job_details = Job_details.split(\"\\n\")\n",
    "                    Job_details = [i.strip() for i in Job_details if i.strip() != \"\"]\n",
    "                else:\n",
    "                    Job_details = []\n",
    "                key_mapping = {\"seniority level\" : \"seniority_level\", \n",
    "                               \"employment type\" : \"employment_type\", \"job function\":\"job_function\"} \n",
    "\n",
    "                for i in range(0, len(Job_details), 2):\n",
    "                    original_key = Job_details[i].lower() \n",
    "                    value = Job_details[i + 1] \n",
    "                    if original_key in key_mapping:\n",
    "                        desired_key = key_mapping[original_key] \n",
    "                    else:\n",
    "                        desired_key = original_key\n",
    "\n",
    "                    job_dict[desired_key] = str(value)\n",
    "\n",
    "                canonical_link = urljoin(job_link, urlparse(job_link).path)\n",
    "                job_dict[\"job_link\"] = canonical_link \n",
    "\n",
    "                job_dict[\"source\"] = self.source\n",
    "                \n",
    "                job_dict['searched_key_ward'] = key_words\n",
    "                return job_dict\n",
    "\n",
    "            except Exception as e:\n",
    "                self.nt_parsed_link.append(job_link)\n",
    "                print(f\"An error occurred:  - {str(e)}\")\n",
    "                retries += 1\n",
    "                # print(f\"Retrying ({retries}/{max_retries}) for {job_link}\")\n",
    "            \n",
    "                try:\n",
    "                    self.browser.get(job_link)\n",
    "                    time.sleep(2)\n",
    "                except Exception as retry_error:\n",
    "                    # print(f\"Error retrying: {str(retry_error)}\")\n",
    "                    pass         \n",
    "        # print(f\"Max retries reached for {job_link}. unable to scrape data.\")\n",
    "        return None\n",
    "           \n",
    "    #_____________________________________________________________________________________________________#\n",
    "\n",
    "    def is_functionality_1_applicable(self):\n",
    "        try:\n",
    "            self.browser.find_element(By.CLASS_NAME,\"contextual-sign-in-modal__modal-dismiss\")\n",
    "            return True\n",
    "        except NoSuchElementException:\n",
    "            return False\n",
    "    #______________________________________________________________________________________________________#\n",
    "\n",
    "    def cancel_popup(self):\n",
    "        try:\n",
    "            wait = WebDriverWait(self.browser,2)\n",
    "            cancel_button = wait.until(EC.presence_of_element_located((By.CLASS_NAME,\"contextual-sign-in-modal__modal-dismiss\")))\n",
    "            cancel_button.click()\n",
    "        except Exception as e:\n",
    "            pass   \n",
    "    #________________________________________________________________________________________________________#\n",
    "\n",
    "    def hit_see_job(self):\n",
    "        try:\n",
    "            wait = WebDriverWait(self.browser,2)\n",
    "            cancel_button = wait.until(EC.presence_of_element_located((By.CLASS_NAME,\"top-card-layout__cta\")))\n",
    "            cancel_button.click()\n",
    "        except Exception as e:\n",
    "            pass \n",
    "    #__________________________________________________________________________________________________________#\n",
    "\n",
    "    def click_cancel_button(self):\n",
    "        try:\n",
    "            button_class_name = \"cta-modal__dismiss-btn\"\n",
    "            wait = WebDriverWait(self.browser, 2)\n",
    "            cancel_button = wait.until(EC.presence_of_element_located((By.CLASS_NAME, button_class_name)))\n",
    "            cancel_button.click()\n",
    "        except Exception as e:\n",
    "            pass  \n",
    "    #____________________________________________________________________________________________________________#\n",
    "\n",
    "    def save_to_csv(self, filename='linkedIN_data.csv'):\n",
    "        if not os.path.isfile(filename):\n",
    "            self.job_data_df.to_csv(filename, index=False)\n",
    "        else:\n",
    "            self.job_data_df.to_csv(filename, mode='a', header=False, index=False)\n",
    "\n",
    "    #____________________________________________________________________________________________________________#\n",
    "\n",
    "    def find_search_bar_with_retry(self, max_retries=random.randint(5,7), delay_between_retries=3):\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                search_bar = self.browser.find_element(By.XPATH,\"//input[@id='job-search-bar-keywords']\")\n",
    "                return search_bar\n",
    "            except NoSuchElementException:\n",
    "                print(f\"Search bar not found. Retrying ({retries + 1}/{max_retries})...\")\n",
    "                retries += 1\n",
    "                time.sleep(delay_between_retries)\n",
    "\n",
    "        # print(\"Max retries reached. Unable to find the search bar.\")\n",
    "        return None\n",
    "\n",
    "    #_____________________________________________________________________________________________________________#\n",
    "\n",
    "    def scrape_and_store_batches(self, key_words, locations):\n",
    "        job_links = []\n",
    "        \n",
    "        for key in key_words:\n",
    "            for loc in locations:\n",
    "                retries = 0\n",
    "                max_retries = random.randint(4,7)\n",
    "                while retries < max_retries:\n",
    "                    try:\n",
    "                        last_height = self.browser.execute_script(\"return document.body.scrollHeight\")\n",
    "                        self.browser.get(base_url)\n",
    "                        print(f'started scrapping for url : {key} with location {loc}' )\n",
    "                        self.browser.maximize_window()\n",
    "                        time.sleep(2)\n",
    "                        try:\n",
    "                            self.click_cancel_button()\n",
    "                            self.cancel_popup()\n",
    "                        except NoSuchElementException:\n",
    "                            pass\n",
    "                        try:\n",
    "                            # Retry finding the search bar with a maximum of 5 retries\n",
    "                            search_bar = self.find_search_bar_with_retry()\n",
    "                            if not search_bar:\n",
    "                                raise NoSuchElementException(\"Search bar not found.\")\n",
    "\n",
    "                            search_bar.clear()\n",
    "                            search_bar.send_keys(key)\n",
    "\n",
    "                            search_bar_location = self.browser.find_element(By.XPATH , \"//input[@id='job-search-bar-location']\")\n",
    "                            search_bar_location.clear()\n",
    "                            search_bar_location.send_keys(loc)\n",
    "\n",
    "                            button = self.browser.find_element(By.XPATH , \"//button[@data-tracking-control-name='public_jobs_jobs-search-bar_base-search-bar-search-submit']\")\n",
    "                            button.click()\n",
    "                            time.sleep(8)\n",
    "                            self.browser.execute_script(f\"window.scrollBy(0, {50});\")\n",
    "                            # self.browser.save_screenshot(f'{key}, {loc}.png')\n",
    "\n",
    "                            print('scrolling')\n",
    "                            \n",
    "                            while True:\n",
    "                                self.browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                                time.sleep(7)\n",
    "                                try:\n",
    "                                    button = WebDriverWait(self.browser, 2).until(\n",
    "                                        EC.visibility_of_element_located((By.CLASS_NAME, self.target_button_class)))\n",
    "                                    button.click()\n",
    "                                    time.sleep(7)\n",
    "                                except:\n",
    "                                    pass\n",
    "                                new_height = self.browser.execute_script(\"return document.body.scrollHeight\")\n",
    "                                if new_height == last_height:\n",
    "                                    break\n",
    "                                last_height = new_height\n",
    "                                \n",
    "                            # self.browser.save_screenshot(f'001_{key},{loc}.png')    \n",
    "\n",
    "                            time.sleep(2)\n",
    "                            html_list = self.browser.find_element(By.CLASS_NAME , 'jobs-search__results-list')\n",
    "                            job_links_list = html_list.find_elements(By.CSS_SELECTOR , 'div a')\n",
    "                            for link in job_links_list:\n",
    "                                if '/jobs/view/' in link.get_attribute('href'):\n",
    "                                    link = link.get_attribute('href')\n",
    "                                    canonical_link = urljoin(link, urlparse(link).path)\n",
    "                                    job_links.append(canonical_link)\n",
    "                            print(f'Number of job links collected for {key} with location {loc} : {len(list(set(job_links)))}')\n",
    "\n",
    "                            for url in list(set(job_links)):\n",
    "                                print(f'started scraping for job_link....')\n",
    "                                try:\n",
    "                                    self.browser.get(url)\n",
    "                                    self.browser.maximize_window()\n",
    "                                    time.sleep(5)\n",
    "\n",
    "                                    func_01 = self.is_functionality_1_applicable()\n",
    "                                    if func_01 == False:\n",
    "                                        self.browser.execute_script(f\"window.scrollBy(0, {50});\")\n",
    "                                        try:\n",
    "                                            self.click_cancel_button()\n",
    "                                        except NoSuchElementException:\n",
    "                                            pass\n",
    "                                        time.sleep(3)\n",
    "                                        job_data = self.job_scrapper(url,key)\n",
    "                                        if job_data:\n",
    "                                            self.job_data_df = pd.DataFrame([job_data],\n",
    "                                                                             columns=self.job_data_df.columns).reindex(\n",
    "                                                columns=self.job_data_df.columns, fill_value=None)\n",
    "                                            self.save_to_csv()\n",
    "#                                             self.push_to_elasticsearch(self.job_data_df)\n",
    "                                    else:\n",
    "                                        try:\n",
    "                                            self.cancel_popup()\n",
    "                                        except NoSuchElementException:\n",
    "                                            pass\n",
    "                                        time.sleep(2)\n",
    "                                        self.browser.execute_script(f\"window.scrollBy(0, {50});\")\n",
    "                                        time.sleep(2)\n",
    "                                        self.hit_see_job()\n",
    "                                        time.sleep(3)\n",
    "                                        try:\n",
    "                                            self.click_cancel_button()\n",
    "                                        except NoSuchElementException:\n",
    "                                            pass\n",
    "                                        random_sleep = random.randint(7, 25)\n",
    "                                        time.sleep(random_sleep)\n",
    "                                        job_data = self.job_scrapper(url,key)\n",
    "\n",
    "                                        if job_data:\n",
    "                                            self.job_data_df = pd.DataFrame([job_data],\n",
    "                                                                             columns=self.job_data_df.columns).reindex(\n",
    "                                                columns=self.job_data_df.columns, fill_value=None)\n",
    "                                            self.save_to_csv()\n",
    "#                                             self.push_to_elasticsearch(self.job_data_df)\n",
    "                                        else:\n",
    "                                            print(\"Both methods failed to scrape job data\")\n",
    "\n",
    "                                    self.links_processed += 1\n",
    "\n",
    "                                    if self.links_threshold != 0 and self.links_processed % self.links_threshold == 0:\n",
    "                                        print(\n",
    "                                            f\"Processed {self.links_processed} links. Taking a pause for {self.pause_duration/60} minutes.\")\n",
    "                                        time.sleep(self.pause_duration)\n",
    "\n",
    "                                except Exception as e:\n",
    "                                    print(f\"Error while scraping {url}: {str(e)}\")\n",
    "                            job_links.clear()\n",
    "                            print(f\"Clearing previous links of location search {key} and location {loc} from the list\")\n",
    "                            print(f'scrapping pause for {self.pause_duration} sec')\n",
    "                            time.sleep(self.pause_duration)\n",
    "\n",
    "                            break\n",
    "\n",
    "                        except NoSuchElementException as e:\n",
    "                            # print(f\"Element not found for {base_url}: {str(e)}\")\n",
    "                            retries += 1\n",
    "                            print(f\"Retrying ({retries}/{max_retries}) for {key}\")\n",
    "                            continue\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"An error occurred for {base_url}: {str(e)}\")\n",
    "                        retries += 1\n",
    "                        print(f\"Retrying ({retries}/{max_retries}) for {key}\")\n",
    "                        \n",
    "                        \n",
    "scrapper = linked_in_JobScraper()\n",
    "\n",
    "key_wards = job_categories = [\n",
    "#     \"Manufacturing\",\n",
    "#     \"Production\",\n",
    "#     \"Management\",\n",
    "#     \"Project Management\",\n",
    "#     \"Purchasing\",\n",
    "#     \"Analyst\",\n",
    "#     \"Consulting\",\n",
    "#     \"Education\",\n",
    "#     \"Design\",\n",
    "#     \"Product Management\",\n",
    "#     \"Business Development\",\n",
    "#     \"General Business\",\n",
    "#     \"Art/Creative\",\n",
    "#     \"Research\",\n",
    "#     \"Distribution\",\n",
    "#     \"Strategy/Planning\",\n",
    "#     \"Advertising\",\n",
    "#     \"Science\",\n",
    "#     \"Public Relations\",\n",
    "#     \"Writing/Editing\",\n",
    "    '\"Training\" or \"trainer\"'\n",
    "]\n",
    "\n",
    "locations = locations = ['United States','Australia','Canada','United Kingdom',\n",
    "                         'India','New Zealand','Saudi Arabia','Singapore','United Arab Emirates']\n",
    "job_links = scrapper.scrape_and_store_batches(key_wards,locations) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a52456d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
